{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now You Code In Class: \n",
    "\n",
    "## Tricks of The Pandas Masters Volume II\n",
    "\n",
    "Once again, we will try something a bit different for our Activity - A series of Pandas coding challenges!\n",
    "\n",
    "Datasets we will use:\n",
    "\n",
    "- Reddit Data: https://raw.githubusercontent.com/mafudge/datasets/master/json-samples/reddit.json\n",
    "- Episodes of the HBO series \"The Wire\": https://raw.githubusercontent.com/mafudge/datasets/master/tv-shows/the-wire.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deserializing json\n",
    "\n",
    "the following function takes a URL as input and returns deserialized json as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_json(url: str):\n",
    "    import requests\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example deserializes reddit news and then finds the key where the articles are and displays the first artcle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reddit = get_json(\"https://raw.githubusercontent.com/mafudge/datasets/master/json-samples/reddit.json\")\n",
    "articles = reddit[\"data\"][\"children\"]\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT 1 read in episodes of \"The Wire\" find the episodes key, and show the first episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataframe\n",
    "\n",
    "In this example we use`json_normalize()` to display the articles in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_df = pd.json_normalize(articles) #articles is a list of dict!\n",
    "art_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT 2 display the episodes as a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Text Sentiment Web Service\n",
    "\n",
    "The following function uses the http://text-processing.com service to calculate the sentiment for any input text. The result is a dict with probabilities and overall sentiment. For example:\n",
    "\n",
    "INPUT: `\"very nice\"`\n",
    "\n",
    "OUTPUT: \n",
    "```\n",
    "{\n",
    "    'probability': {\n",
    "        'neg': 0.28997418956645504,\n",
    "        'neutral': 0.13591527211268692,\n",
    "        'pos': 0.710025810433545\n",
    "    },\n",
    "    'label': 'pos'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probability': {'neg': 0.28997418956645504,\n",
       "  'neutral': 0.13591527211268692,\n",
       "  'pos': 0.710025810433545},\n",
       " 'label': 'pos'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_sentiment(text: str) -> dict:\n",
    "    import requests\n",
    "    response = requests.post(\n",
    "        \"http://text-processing.com/api/sentiment/\", \n",
    "        data={\"text\": text},\n",
    "        headers={\"User-Agent\": \"Mozillia/5.0\"}\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "sentiment = get_text_sentiment(\"very nice\")\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probability': {'neg': 0.6701191630651346,\n",
       "  'neutral': 0.0828566177589778,\n",
       "  'pos': 0.3298808369348654},\n",
       " 'label': 'neg'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PROMPT 3: get the sentiment of the text below and print the label only\n",
    "text = \"The layover for my flight was very long and it was cold on the plane and I had a stomach ache\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a lambda with `apply()`\n",
    "\n",
    "The following example creates a new Series in the dataframe called `\"title_sentiment\"` which calculates the sentiment of the reddit news title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_df[\"title_sentiment\"] = art_df.apply(lambda row: get_text_sentiment(row[\"data.title\"]), axis=1)\n",
    "art_df[[\"data.title\", \"title_sentiment\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPT 4 get the sentiment for the summary Series in the episodes dataframe, name the column \"summary_sentiment\" and show the summary and the sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving the dataframe \n",
    "\n",
    "Calculating the sentiment isn't free and we don't need to do it more than once. Its a good idea to save the content of our dataframe at this point.  That way we can continue from the saved file versus re-calculating the transformation.\n",
    "\n",
    "After you run this code open the `reddit_news_articles.csv` file and check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "art_df.to_csv(\"reddit_news_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPT 5 save your episodes of the wire to \"the_wire_episodes.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hot and Top articles.\n",
    "\n",
    "You have been hired as a data scientist at Reddit. Through your extensive statistical research, you have determined: \n",
    "\n",
    "- Articles where the number of comments are in the 75% percentile should be categorized as `\"Hot\"`\n",
    "- Articles where the number of upvotes are in the 75% percentile should be categorized as `\"Popular\"`\n",
    "- Articles matching both should be categories as `\"Fire\"`\n",
    "\n",
    "In these next challenges, we will write code to implement these rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPT 6 write code to show the statistics of the numerical columns in the `art_df`\n",
    "art_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT 7 which columns represent comments? upvotes? What are the thresholds based on the 75% percentile?\n",
    "hot_comments_minimum = art_df.describe().loc[\"75%\", \"data.num_comments\"]\n",
    "pop_comments_minimum = art_df.describe().loc[\"75%\", \"data.ups\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT 8 Write code to extract the comments_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPT 9 Write code to extract the upvote_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining the `categorize()` function:\n",
    "\n",
    "Let's review the requirements: \n",
    "\n",
    "- Articles where the number of comments are in the 75% percentile should be categorized as \"Hot\"\n",
    "- Articles where the number of upvotes are in the 75% percentile should be categorized as \"Popular\"\n",
    "- Articles matching both should be categories as \"Fire\"\n",
    "\n",
    "INPUTS (4): \n",
    "\n",
    "    PROMPT 10\n",
    "    \n",
    "OUTPUTS (1):\n",
    "\n",
    "    PROMPT 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPT 12 write function definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPT 13a write test for \"Hot\"\n",
    "upvotes = 0\n",
    "upvote_threshold = 0\n",
    "comments = 0\n",
    "comments_threshold = 0\n",
    "expect = \"????\"\n",
    "actual = categorize(upvotes, upvote_threshold, comments, comments_threshold)\n",
    "print(f\"When upvotes={upvotes}/{upvote_threshold}, comments={comments}/{comments_threshold}, EXPECT={expect}, ACTUAL={actual}\")\n",
    "assert expect == actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPT 13b write test for \"Popular\"\n",
    "upvotes = 0\n",
    "upvote_threshold = 0\n",
    "comments = 0\n",
    "comments_threshold = 0\n",
    "expect = \"????\"\n",
    "actual = categorize(upvotes, upvote_threshold, comments, comments_threshold)\n",
    "print(f\"When upvotes={upvotes}/{upvote_threshold}, comments={comments}/{comments_threshold}, EXPECT={expect}, ACTUAL={actual}\")\n",
    "assert expect == actua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPT 13c write test for \"Fire\"\n",
    "upvotes = 0\n",
    "upvote_threshold = 0\n",
    "comments = 0\n",
    "comments_threshold = 0\n",
    "expect = \"????\"\n",
    "actual = categorize(upvotes, upvote_threshold, comments, comments_threshold)\n",
    "print(f\"When upvotes={upvotes}/{upvote_threshold}, comments={comments}/{comments_threshold}, EXPECT={expect}, ACTUAL={actual}\")\n",
    "assert expect == actua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROMPT 13d write test for \"\"\n",
    "upvotes = 0\n",
    "upvote_threshold = 0\n",
    "comments = 0\n",
    "comments_threshold = 0\n",
    "expect = \"????\"\n",
    "actual = categorize(upvotes, upvote_threshold, comments, comments_threshold)\n",
    "print(f\"When upvotes={upvotes}/{upvote_threshold}, comments={comments}/{comments_threshold}, EXPECT={expect}, ACTUAL={actual}\")\n",
    "assert expect == actua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step. lambda / apply\n",
    "\n",
    "Add a column to the dataframe called `\"category\"` which calculates the category for the article \n",
    "\n",
    "output the article title, the number of comments, number of upvotes and the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
